\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{amsmath}
\usepackage{ dsfont }

\title{Вариационные Автоенкодеры с мультиномиальным Prior'ом.}
\author{Денис Мазур}
\date{Ноябрь 2018}

\begin{document}

\maketitle

\begin{abstract}
    Метод вариационных автоэнкодеров является, вероятно, самым эффективным
    среди методов глубинного обучения без учителя...
\end{abstract}

\tableofcontents

\section{Вступление}
Так как прочтение данной работы не предпологает ознакомленность с методами машинного обучения, будет дан поверхностый разбор некоторых ключевых принципов и методов,
необходимых для понимания вариационных автоенкодеров. 

От читателя предполагается понимание математического анализа, линейной алгебры и теории вероятности.

\section{Обучение нейронных сетей}
Для базового понимания автоенкодеров не требуется понимание принципа работы нейронных сетей, по этой причине объяснение устройства нейронных сетей было решено опустить 
и уделить больше внимания принципу их обучения. Для интересующихся, литература на тему нейронных сетей будет приложина в библиографии.

Для простоты устройсто нейронной сети можно редуцировать до некоторой функции, имеющей параметы $W$, принимающей значения $X$ и выдающей значения $\widehat{X}$. 
Физический смысл $X$ - данные, подаваемые на вход сети, $\widehat{X}$ - выход нейронной сети. Каждому элементу $x \in X$ соответсвует элемент $y \in Y$, и задачей обучения
нейронной сети является подобрать параметы $W$ нейронной сети так, чтобы они минимально или не отличались от $Y$. Также можно сформулировать задачу следущим образом:

$$d(f(x_n), y_n) \rightarrow \text{min}$$

Также, можно поставить задачу как поиск оптмальных параметров $W$:

$$\underset{W}{\textbf{argmin }} d(f(x_n), y_n)$$ 

Где $f(x)$ - нейронная сеть, а $d(x, y)$ определенная нами функция расстояния (отличия) между выходом нейронной сети и настоящим ответом, называемая функцией потерь.
Важно понимать следующие свойства функций $f(x)$ и $d(x, y)$, они обе дифференцируемые и определены на всех $X$, а это значит, что мы к параметрам нейронной сети $W$ можем
применять любые методы оптимизации функций, например, градиентный метод.

\subsection{Градиентный метод}
Градиентый метод является методом решения задач вида $\underset{a \in \mathds{R}^n}{\textbf{min }} g(x)$. Алгоритм градиентного метода начинается с выбора
параметоров $a$, это может делать как и из какого либо представления того какими примерно эти парамерты должны быть, так и случайным образом. Далее
идет так называемый "шаг" градиентного метода. Из параметров $a$ вычитаеся значение градента функции $g(a)$ в точке текущего значения $a$:

$$ a_{n + 1} = a_n - \nabla g(a_n)$$

Чтобы не "перескочить" через минимум функции во время итерации метода, градиент $\nabla f(x)$ обычно умножается на какую-то константу $\alpha$:

$$ a_{n + 1} = a_n - \alpha \nabla g(a_n)$$

\subsection{Градиентный метод в обучении нейронных сетей}
Теперь о том как градиентный метод может применяться в обучении нейронных сетей.

Функцию потерь нейронной сети $d(f(x_n), y_n)$ можно записать в виде, $d(f(x_n, W), y_n)$, до этого, для простоты, записи парамерты нейронной сети $W$ не записывались
как аргумент, хотя, очевидно, им являются. Поясню, что X и Y являются константными значениями, так как являются выборкой наших
данных и известны нам заранее. Таким образом, единственный аргумент, который мы можем изменять, минимизируя функцию потерь - $W$.

На практике, оптимизировать функцию потерь градиентным спуском "в лоб" не получится. У этого есть несколько причин, одна из главных - риск переобучения
модели (ситуация при которой модель показывает идеальных результат на обучающей выборке, но плохой на валидации). Но помимо этого, выборка данных
чаще всего слишком велика и не помещается в память компьютера, по этой причине на каждой итерации градиентного метода выбирается случайная подвыборка
тренировочных данных и над ней совершается шаг граиентого метода. Данный метод называется стохастическим градиентным спуском.

\section{Автоенкодеры}
Автоенкодеры являются архитектурой нейронных сетей, использующейся приемущественно для кодирования многомерных данных в данные меньшей
размерности. Автоенкодеры состоят из двух отдельных нейросетей, первая - енкодер, преобразующая исходные данные в с сжатую репрезентацию себя. 
Обозначим енкодер как $e(x) \rightarrow z$. Вторая - декодер, преобращующая сжатую репрезентацию данных $z$ в их первоначальную форму. Обозначим декодер
как $d(z) \rightarrow \widehat{x}$. 

$$e(x) = z$$
$$\lVert z \rVert << \lVert x \rVert $$
$$d(e(x)) \approx x$$

\subsection{Почему это работает?}
Архитектура автоенкодеров исходит из того, что изначальные данные содержат больше информации, чем необходимо для их кодирования, а значит при
их кодировании мы не теряем данные. Это становится интуитвно понятно, если представить себе набор данных из изображений геометрических фигур.
Так как для их кодирования нам надо знать лишь вид фигуры, цвет и положение.


\section{Вариационные автоенкодеры}
Есть два подхода к понимаю вариационных автоенкодеров. Один несколько более инженерный и практичный, другое более филосовски-идеоматический
с точки зрения статистики и Байесовского машинного обучения. Эта работа сфокусируется на втором по той причине, что он дает большее понимание того,
что мы хотим достичь нашей работой, а также дает более полное понимание их принципе работы.

\end{document}